{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center><img src=\"images/logo.png\" alt=\"AWS Logo\" width=\"400\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n",
    "\n",
    "# Application of Deep Learning to Text and Image Data\n",
    "## Module 1, Lab 4: Introducing CNNs\n",
    "\n",
    "In the previous labs, you used neural networks to predict the target field of a given dataset. You used a feed-forward neural network for a multiclass classification task using images as inputs.\n",
    "\n",
    "Now you will use a convolutional neural network (CNN) that is specialized to extract useful information from images. You will train and evaluate this network on a dataset of handwritten digits, and you will try to predict a number that is represented in an image.\n",
    "\n",
    "You will learn how to do the following:\n",
    "\n",
    "- Build a CNN.\n",
    "- Train a CNN.\n",
    "- Test the performance of a CNN.\n",
    "\n",
    "---\n",
    "\n",
    "You will be presented with two kinds of exercises throughout the notebook: activities and challenges. <br/>\n",
    "\n",
    "| <img style=\"float: center;\" src=\"images/activity.png\" alt=\"Activity\" width=\"125\"/>| <img style=\"float: center;\" src=\"images/challenge.png\" alt=\"Challenge\" width=\"125\"/>|\n",
    "| --- | --- |\n",
    "|<p style=\"text-align:center;\">No coding is needed for an activity. You try to understand a concept, <br/>answer questions, or run a code cell.</p> |<p style=\"text-align:center;\">Challenges are where you can practice your coding skills.</p> |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Index\n",
    "\n",
    "* [MNIST dataset](#MNIST-dataset)\n",
    "* [Creating a CNN](#Creating-a-CNN)\n",
    "* [Training the network](#Training-the-network)\n",
    "* [Testing the network](#Testing-the-network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## MNIST dataset\n",
    "\n",
    "The [MNIST dataset](http://yann.lecun.com/exdb/mnist) is a large collection of handwritten digits. Each example contains a pixel map showing how a person wrote a digit. The images have been size-normalized and centered with fixed dimensions. The labels correspond to the digit in the image, ranging from 0 to 9. This is a multiclass classification task with 10 output classes.\n",
    "\n",
    "<img src=\"images/MnistExamples.png\" alt=\"MNIST Examples\" />\n",
    "\n",
    "First, download the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install libraries\n",
    "!pip install -U -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library dependencies\n",
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: [60000, 28, 28]. \n",
      "Test data shape: [10000, 28, 28]\n"
     ]
    }
   ],
   "source": [
    "# Load the train data (it's included in the torchvision library)\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root=\"data\", train=True, transform=transforms.ToTensor(), download=True\n",
    ")\n",
    "\n",
    "# Load the test data (it's included in the torchvision library)\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root=\"data\", train=False, transform=transforms.ToTensor(), download=True\n",
    ")\n",
    "\n",
    "# Print the dimensions of the datasets\n",
    "print(\n",
    "    \"Training data shape: {}. \\nTest data shape: {}\".format(\n",
    "        list(train_data.data.shape), list(test_data.data.shape)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <h3><i>Try it yourself!</i></h3>\n",
    "    <br>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"images/activity.png\" alt=\"Activity\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\">To observe a sample image from the MNIST dataset, run the following cell. The image is labeled with the digit that is present in the sample image.</p>\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Image with target: 2')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show an example image\n",
    "plt.imshow(train_data.data[29], cmap=\"gray\")\n",
    "plt.title(\"Image with target: %i\" % train_data.targets[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Creating a CNN\n",
    "\n",
    "Convolutional neural networks (CNNs) are popular with image data. The network automatically extracts useful features from images, such as edges, contours, and objects.\n",
    "\n",
    "This lab introduces CNNs, but the details of CNNs will be discussed in a later module.\n",
    "\n",
    "CNNs require minimal preprocessing compared to older algorithms, such as feed-forward neural networks, that are used for computer vision. Although feed-forward neural networks can still be used with image data, CNNs can capture the spatial and temporal properties in an image with a significant reduction in the number of parameters. In this notebook, you will use a simple CNN to extract information from image data.\n",
    "\n",
    "You will use PyTorch's [Conv2D layer](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) with the following interface to process the images:\n",
    "\n",
    "`nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, ...) `\n",
    "\n",
    "Parameter definitions:\n",
    "* __in\\_channels (int):__ Number of channels in the input image\n",
    "* __out\\_channels (int):__ Number of channels that are produced by the convolution\n",
    "* __kernel\\_size (int or tuple):__ Size of the convolving kernel\n",
    "* __stride (int or tuple, optional):__ Stride of the convolution (default is 1)\n",
    "\n",
    "The output dimension of the Conv2D layer can be calculated using the following formula:\n",
    "\n",
    "`((W - K + 2P)/S + 1)`\n",
    "\n",
    "Where:\n",
    "- W = Input size\n",
    "- K = Kernel size\n",
    "- S = Stride\n",
    "- P = Padding (not used in the notebook)\n",
    "\n",
    "Example: \n",
    "\n",
    "For an `image of size = (28x28)`, `kernel size = 3` , `stride = 1`, and `padding = 0`, the output dimension is `(28 - 3 + 0)/1 + 1 = 26`. \n",
    "\n",
    "With `out_channels = 1`, the output dimension is `(26, 26)`.\n",
    "\n",
    "With `out_channels = 3`, the output dimension is `(26, 26, 3)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "batch_size = 100  # Size of input data for one iteration\n",
    "num_classes = 10  # Number of output classes, discrete range [0,9]\n",
    "num_epochs = (\n",
    "    10  # Number of times that the entire dataset is passed through the model\n",
    ")\n",
    "\n",
    "# Size of step\n",
    "lr = 1e-3\n",
    "\n",
    "# Use GPU if available; otherwise, use CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Use PyTorch DataLoaders to load the data in batches\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_data, batch_size=batch_size, shuffle=True, drop_last=True\n",
    ")\n",
    "\n",
    "# Repeat for test dataset\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_data, batch_size=batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\"> \n",
    "  <h3><i>Try it yourself!</i></h3>\n",
    "  <p style=\"text-align:center; margin:auto;\"><img src=\"images/challenge.png\" alt=\"Challenge\" width=\"100\" /> </p>\n",
    "  <p style=\" text-align: center; \">Create a neural network with a 2D convolutional layer and the following attributes:</p>\n",
    "  <div style=\"width:600px; margin: 0 auto;\">\n",
    "    <ul style=\"text-align:left\">\n",
    "    <li>Conv2D layer with <code>in_channel=1</code>, <code>out_channel=32</code>, and <code>kernel_size=3</code></li>\n",
    "    <li>Flatten the layer to squash the data into a one-dimensional tensor</li>\n",
    "    <li>Linear layer with 128 units</li>\n",
    "    <li>One output layer</li>\n",
    "    <li>Softmax activation function for the output layer</li>\n",
    "    </ul>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): Flatten(start_dim=1, end_dim=-1)\n",
       "  (3): Linear(in_features=21632, out_features=128, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (6): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 26 * 26 * 32  # Flattened dimension for the linear layer\n",
    "\n",
    "############### CODE HERE ###############\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(start_dim=1),\n",
    "    nn.Linear(input_size, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, num_classes),\n",
    "    nn.Softmax(dim=1),).to(device)\n",
    "\n",
    "############## END OF CODE ##############\n",
    "\n",
    "def xavier_init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "# Initialize weights/parameters for the network\n",
    "net.apply(xavier_init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the loss function and the optimizer\n",
    "\n",
    "# Choose cross-entropy loss for this classification problem\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# Choose the Adam optimizer. You can also experiment with other optimizers.\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Training the network\n",
    "\n",
    "Now you are ready to train the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train_loss 1.865907 Seconds 24.125017\n",
      "Epoch 1. Train_loss 1.493767 Seconds 6.397725\n",
      "Epoch 2. Train_loss 1.482056 Seconds 6.390371\n",
      "Epoch 3. Train_loss 1.477057 Seconds 6.383992\n",
      "Epoch 4. Train_loss 1.473741 Seconds 6.421503\n",
      "Epoch 5. Train_loss 1.471021 Seconds 6.412659\n",
      "Epoch 6. Train_loss 1.469994 Seconds 6.399873\n",
      "Epoch 7. Train_loss 1.469356 Seconds 6.414932\n",
      "Epoch 8. Train_loss 1.467707 Seconds 6.378420\n",
      "Epoch 9. Train_loss 1.466567 Seconds 6.354318\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Network training and validation\n",
    "\n",
    "# Start the outer epoch loop (epoch = full pass through the dataset)\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    training_loss = 0.0\n",
    "\n",
    "    # Training loop (with autograd and trainer steps)\n",
    "    # This loop trains the neural network\n",
    "    # Weights are updated here\n",
    "    net.train()  # Activate training mode (dropouts and so on)\n",
    "    for images, target in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        images = images.to(device)\n",
    "        target = target.to(device)\n",
    "        # Forward + backward + optimize\n",
    "        output = net(images)\n",
    "        L = loss(output, target)\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        # Add batch loss\n",
    "        training_loss += L.item()\n",
    "\n",
    "    # Take the average losses\n",
    "    training_loss = training_loss / len(train_loader)\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Epoch %s. Train_loss %f Seconds %f\" % (epoch, training_loss, end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Testing the network\n",
    "\n",
    "Finally, evaluate the performance of the trained network on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       980\n",
      "           1       0.99      1.00      0.99      1135\n",
      "           2       0.98      0.98      0.98      1032\n",
      "           3       0.99      0.99      0.99      1010\n",
      "           4       0.98      0.99      0.99       982\n",
      "           5       0.99      0.98      0.98       892\n",
      "           6       0.99      0.99      0.99       958\n",
      "           7       0.98      0.97      0.98      1028\n",
      "           8       0.98      0.97      0.98       974\n",
      "           9       0.99      0.97      0.98      1009\n",
      "\n",
      "    accuracy                           0.98     10000\n",
      "   macro avg       0.98      0.98      0.98     10000\n",
      "weighted avg       0.98      0.98      0.98     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "net.eval()  # Activate eval mode (don't use dropouts and such)\n",
    "\n",
    "# Get test predictions\n",
    "predictions, labels = [], []\n",
    "for images, target in test_loader:\n",
    "    images = images.to(device)\n",
    "    target = target.to(device)\n",
    "\n",
    "    predictions.extend(net(images).argmax(axis=1).tolist())\n",
    "    labels.extend(target.tolist())\n",
    "\n",
    "# Print performance on the test data\n",
    "print(classification_report(labels, predictions, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, you practiced using a CNN. \n",
    "\n",
    "--- \n",
    "## Next Lab: Processing text\n",
    "In the next lab you will learn how to do more advanced text processing."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
