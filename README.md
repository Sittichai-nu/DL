## Sittichai Chaikamol
### ITAI 2376 - Deep Learning
**Professor:** Patricia McManus

## Introduction
This portfolio encapsulates my journey through the Deep Learning course, highlighting key learnings, activities, and reflections across various modules, emphasizing both the theoretical and practical applications of deep learning technologies.

### Course Overview
The course provided a comprehensive exploration of artificial intelligence technologies focusing on deep learning, machine learning algorithms, neural networks, NLP, and real-world applications, aimed at equipping students with the necessary skills to implement AI systems.

### Personal Learning Objectives
- Gain a deep understanding of Deep Learning fundamentals and state-of-the-art technologies.
- Develop hands-on proficiency in applying AI techniques to solve complex problems.
- Explore the ethical implications of AI and promote sustainable and responsible AI development.

## Table of Contents
- [Introduction](#introduction)
- [Module Summaries](#module-summaries)
  - [Module 1: Introduction to Deep Learning](#module-1-introduction-to-deep-learning)
  - [Module 2: Gradient-Based Learning and Deep Learning Toolkits](#module-2-gradient-based-learning-and-deep-learning-toolkits)
  - [Module 3: Neural Network Basics](#module-3-neural-network-basics)
  - [Module 4: Tensors and Deep Learning Frameworks](#module-4-tensors-and-deep-learning-frameworks)
  - [Module 5: Convolutional Neural Networks (CNNs)](#module-5-convolutional-neural-networks)
  - [Module 6: Advanced Neural Network Architectures](#module-6-advanced-neural-network-architectures)
  - [Module 7: Natural Language Processing (NLP)](#module-7-natural-language-processing)
  - [Module 8: Advanced Text Processing Techniques](#module-8-advanced-text-processing-techniques)
  - [Module 9: Word Embeddings and GloVe Vectors](#module-9-word-embeddings-and-glove-vectors)
  - [Module 10: Recurrent Neural Networks and LSTM](#module-10-recurrent-neural-networks-and-lstm)
  - [Module 11: Transformers and BERT](#module-11-transformers-and-bert)
  - [Module 12: Computer Vision and Image Processing](#module-12-computer-vision-and-image-processing)
  - [Module 13: AI Agents](#module-13-ai-agents)
- [Midterm Project: The Vectorization Adventure](#midterm-project-the-vectorization-adventure)
- [Conclusion and Future Directions](#conclusion-and-future-directions)
- [Acknowledgments](#acknowledgments)

## Module Summaries
Each module explores different aspects of deep learning, from foundational concepts to advanced applications in various fields.

### Module 1: Introduction to Deep Learning
**Key Points Learned:**
- Overview of deep learning's role within AI.
- Fundamental concepts of neural networks and their evolution.

**Activities and Results:**
- Reviewed historical milestones in deep learning.
- Explored critical developments through the "Deep Learning Timeline" infographic.

**Reflection:**
- Appreciated the transformative potential of deep learning technologies.

**[Deep Learning Timeline Analysis](https://github.com/Sittichai-nu/DL/tree/main/Module%201%20-%20Deep%20Learning%20Introduction)**

### Module 2: Gradient-Based Learning and Deep Learning Toolkits
**Key Points Learned:**
- Gradient-based optimization techniques.
- Deep learning frameworks like TensorFlow and MXNet.

**Activities and Results:**
- Comparative analysis of deep learning tools.

**Reflection:**
- Gained a practical understanding of framework capabilities.

**[Comparative Analysis Project](https://github.com/Sittichai-nu/DL/tree/main/Module%202%20-%20Gradient-Based%20Learning%20and%20Deep%20Learning%20Toolkits)**

### Module 3: Neural Network Basics
**Key Points Learned:**
- Basics of neural networks including perceptrons and sigmoid neurons.
- Introduction to backpropagation.

**Activities and Results:**
- Hands-on building and training basic neural network models.
- Experimented with various activation functions.

**Reflection:**
- Understood the challenges in learning algorithms and the importance of proper parameter tuning.

**[Neural Network Basics Workshop](https://github.com/Sittichai-nu/DL/tree/main/Module%203%20-%20Neural%20Network%20Basics)**

### Module 4: Tensors and Deep Learning Frameworks
**Key Points Learned:**
- Introduction to tensors, the fundamental building blocks of data in deep learning.
- Detailed overview of PyTorch and TensorFlow's capabilities.

**Activities and Results:**
- Performed operations on tensors to manipulate data.
- Built simple models in both frameworks to compare usability.

**Reflection:**
- Realized the power of tensor operations in streamlining the creation of complex models.

**[Tensor Manipulation Exercises](https://github.com/Sittichai-nu/DL/tree/main/Module%204%20-%20Tensors%20and%20Deep%20Learning%20Frameworks)**

### Module 5: Convolutional Neural Networks (CNNs)
**Key Points Learned:**
- Fundamentals of CNNs and their applications in image recognition.
- Understanding of different layers like Conv2D, MaxPooling, and fully connected layers.

**Activities and Results:**
- Implemented a CNN to classify images from a public dataset.
- Experimented with various architectures to optimize performance.

**Reflection:**
- Learned the significance of convolutional layers in feature extraction.

**[CNN Image Classification Project](https://github.com/Sittichai-nu/DL/tree/main/Module%205%20-%20Convolutional%20Neural%20Networks%20(CNNs))**

### Module 6: Advanced Neural Network Architectures
**Key Points Learned:**
- Deep dive into architectures like RNNs, LSTMs, and GANs.
- Applications of advanced networks in sequence modeling and generative tasks.

**Activities and Results:**
- Developed an LSTM model for time-series prediction.
- Created a simple GAN to generate new images.

**Reflection:**
- Appreciated the complexity and capabilities of modern neural network architectures.

**[Advanced Architectures Implementation](https://github.com/Sittichai-nu/DL/tree/main/Module%206%20-%20Advanced%20Neural%20Network%20Architectures)**

### Module 7: Natural Language Processing (NLP)
**Key Points Learned:**
- Introduction to NLP and its challenges such as language ambiguity and context sensitivity.
- Overview of text processing techniques and models like BERT.

**Activities and Results:**
- Applied sentiment analysis on social media data.
- Experimented with BERT for question answering systems.

**Reflection:**
- Understood the difficulties of dealing with human language and the potential of NLP in AI.

**[NLP Case Studies](https://github.com/Sittichai-nu/DL/tree/main/Module%207%20-%20Natural%20Language%20Processing%20(NLP))**

### Module 8: Advanced Text Processing Techniques
**Key Points Learned:**
- Techniques for text vectorization such as Bag of Words and TF-IDF.
- Understanding the implications of different text preprocessing methods on model performance.

**Activities and Results:**
- Applied various text vectorization techniques to a dataset of articles for topic classification.
- Explored feature selection and its impact on classification accuracy.

**Reflection:**
- Learned the critical role of text preprocessing in the pipeline of NLP applications.

**[Text Processing Techniques](https://github.com/Sittichai-nu/DL/tree/main/Module%208%20-%20Advanced%20Text%20Processing%20Techniques)**

### Module 9: Word Embeddings and GloVe Vectors
**Key Points Learned:**
- The concept of word embeddings and their importance for representing text data.
- Hands-on with pre-trained GloVe vectors for embedding generation.

**Activities and Results:**
- Implemented word embeddings in a text similarity task.
- Explored the semantic relationships captured by GloVe vectors through practical exercises.

**Reflection:**
- Gained insights into how embeddings capture context and semantics, significantly outperforming traditional vectorization methods.

**[Word Embeddings Workshop](https://github.com/Sittichai-nu/DL/tree/main/Module%209%20-%20Word%20Embeddings%20and%20GloVe%20Vectors)**

### Module 10: Recurrent Neural Networks and LSTM
**Key Points Learned:**
- Principles of Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM) networks.
- Applications of RNNs and LSTMs in sequence prediction problems.

**Activities and Results:**
- Built an LSTM model to predict future stock prices based on historical data.
- Experimented with different configurations of RNNs for natural language generation.

**Reflection:**
- Understood the strengths and limitations of recurrent networks, especially in handling sequences with long dependencies.

**[Sequence Modeling Projects](https://github.com/Sittichai-nu/DL/tree/main/Module%2010%20-%20Recurrent%20Neural%20Networks%20and%20LSTM)**

### Module 11: Transformers and BERT
**Key Points Learned:**
- Introduction to the architecture of Transformers and their mechanism of self-attention.
- Deep dive into BERT and its applications in various NLP tasks.

**Activities and Results:**
- Fine-tuned a BERT model for sentiment analysis on a customer review dataset.
- Explored the transformer models for multi-language translation.

**Reflection:**
- Realized the revolutionary impact of transformers on the field of NLP, offering significant improvements over previous models.

**[Transformers in Action](https://github.com/Sittichai-nu/DL/tree/main/Module%2011%20-%20Transformers%20and%20BERT)**

### Module 12: Computer Vision and Image Processing
**Key Points Learned:**
- Fundamentals of computer vision and techniques for image classification and object detection.
- Advanced topics such as semantic segmentation and the use of CNNs in vision tasks.

**Activities and Results:**
- Developed models for facial recognition and gesture analysis using deep learning.
- Implemented semantic segmentation for scene understanding in autonomous driving simulations.

**Reflection:**
- Appreciated the complexities and capabilities of computer vision systems, especially in interpreting visual information from the real world.

**[Computer Vision Projects](https://github.com/Sittichai-nu/DL/tree/main/Module%2012%20-%20Computer%20Vision%20and%20Image%20Processing)**

### Module 13: AI Agents
**Key Points Learned:**
- Overview of AI agents, including types such as reflex agents, model-based agents, and learning agents.
- Discussion on the application of AI agents in various fields like gaming, autonomous vehicles, and healthcare.

**Activities and Results:**
- Simulated a smart agent for a gaming environment to understand decision-making processes.
- Analyzed the use of AI agents in healthcare for predictive analytics and patient management.

**Reflection:**
- Recognized the potential of intelligent agents in automating tasks and enhancing decision-making across diverse sectors.

**[AI Agents Case Studies](https://github.com/Sittichai-nu/DL/tree/main/Module%2013%20-%20AI%20Agents)**


## Conclusion and Future Directions
**Summary of Learning:**
- Explored various aspects of AI from deep learning frameworks to advanced concepts like AI agents.
- Enhanced practical skills in implementing AI models and solving complex problems.

**Future Directions:**
- Interested in further studies on the ethical implications of AI and contributing to sustainable AI practices.
- Opportunities for future projects and research in optimizing AI models for efficiency and ethical considerations.

**Acknowledgments:**
- Special thanks to Professor Patricia McManus, course instructors, and peers for their guidance and collaboration.

## Midterm Project: The Vectorization Adventure
**Overview:** Focused on text-to-vector transformations essential for NLP.
**Outcome:** Enhanced practical understanding and application of vectorization techniques.
**[Midterm Project](https://github.com/Sittichai-nu/DL/tree/main/Midterm%20Project%20The%20Vectorization%20Adventure)**

## Additional Resources and References
- [Deep Learning Frameworks]()
- [Ethical AI Development]()
